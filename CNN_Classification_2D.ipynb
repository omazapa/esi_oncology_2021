{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "590ddf59",
   "metadata": {},
   "source": [
    "<img style=\"width:20%;float: left; margin-right: 10px;\" src=\"https://upload.wikimedia.org/wikipedia/en/a/ae/CERN_logo.svg\"/>\n",
    "\n",
    "\n",
    "\n",
    "#    2D images classification using Machine Leaning for brain tumors\n",
    "\n",
    "## Introduction\n",
    "Brain tumors are considered by the medical community, an aggressive and difficult to treat disease that affects adults and children.\n",
    "\n",
    "The most common primary brain tumors are Gliomas, followed by Meningiomas and Pituitary.\n",
    "\n",
    "Gliomas are the most common type of adult brain tumor, It is about 78 percent of malignant brain tumors according to the American Association of Neurological Surgeons.<a href=\"#1\">[1]</a>\n",
    "\n",
    "In this tutorial we are going to classify 2D images with machine learning, for brain tumor such as **meningioma**, **glioma** and **pituitary**, respect people with no tumors.\n",
    "\n",
    "As mentioned in the preprocessing notebook, we are using like ilustrative example a public dataset, taken from \n",
    "kaggle <a href=\"#2\">[2]</a>, this is a 2D MRI dataset.\n",
    "\n",
    "\n",
    "## About ML for Brain Tumor Classification\n",
    "\n",
    "Brain tumor classification is complex task, it requies sophistcate machine learning and math models using the newest in the state of the art for image processing like Convolutional Neural Networks <a href=\"#3\">[3]</a>.\n",
    "It also requires elaborated techniques to preprocess the images, such as skull stripping <a href=\"#4\">[4]</a>, advanced tools for data normalization such as Ants <a href=\"#5\">[5]</a>, tools for image transformation such as\n",
    "scikit-image <a href=\"#6\">[6]</a> or open computer vision library (open-cv) <a href=\"#7\">[7]</a>\n",
    "\n",
    "This tutorial is done using Tensorflow with Keras <a href=\"#8\">[8]</a> and due the limited resources will a binary classifier using 64x64 pixels 2D images.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59460cee",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "# Let's get started!\n",
    "\n",
    "The first step is to import the required modules. those modules are to handle numeric arrays, plotting, create the machine learning model and compute some statistics over the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9000cfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "import glob\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Adam #http://arxiv.org/abs/1412.6980\n",
    "from tensorflow.keras.layers import Dense, Conv2D, Flatten, AveragePooling2D, Dropout,BatchNormalization,Activation,SpatialDropout2D\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "from sklearn.metrics import auc\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61772e0b",
   "metadata": {},
   "source": [
    "# Defining and Select the categories\n",
    "\n",
    "The second step is to defined the categories for our problem, in our case is an array of four labels for meningioma, glioma, pituitary tumors and not tumor.\n",
    "\n",
    "We have to select two of those categories to perform the binary classification, initially we will take no tumor and glioma."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b645cbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = ['meningioma_tumor', 'glioma_tumor', 'pituitary_tumor', 'no_tumor']\n",
    "categories_selected = [\"no_tumor\",\"glioma_tumor\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a22ff75e",
   "metadata": {},
   "source": [
    "# Function to plot a confisuion matrix\n",
    "\n",
    "Confusion matrix is a table layout that allows to visualize the performance of the algorithm.<a href=\"#9\">[9]</a>\n",
    "\n",
    "This allows see :\n",
    "* TP: True postive \n",
    "* TN: True negative\n",
    "* FP: False positive\n",
    "* FN: False negative\n",
    "\n",
    "\n",
    "We took this code  snippet from kaggle <a href=\"#10\">[10]</a> to produce a nice plot.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca206ec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm,\n",
    "                          target_names,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=None,\n",
    "                          normalize=True):\n",
    "    accuracy = np.trace(cm) / float(np.sum(cm))\n",
    "    misclass = 1 - accuracy\n",
    "\n",
    "    if cmap is None:\n",
    "        cmap = plt.get_cmap('Blues')\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "\n",
    "    if target_names is not None:\n",
    "        tick_marks = np.arange(len(target_names))\n",
    "        plt.xticks(tick_marks, target_names, rotation=45)\n",
    "        plt.yticks(tick_marks, target_names)\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "\n",
    "    thresh = cm.max() / 1.5 if normalize else cm.max() / 2\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        if normalize:\n",
    "            plt.text(j, i, \"{:0.4f}\".format(cm[i, j]),\n",
    "                     horizontalalignment=\"center\",\n",
    "                     color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "        else:\n",
    "            plt.text(j, i, \"{:,}\".format(cm[i, j]),\n",
    "                     horizontalalignment=\"center\",\n",
    "                     color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label\\naccuracy={:0.4f}; misclass={:0.4f}'.format(accuracy, misclass))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "472ff84e",
   "metadata": {},
   "source": [
    "# Utility funtion to load the dataset\n",
    "\n",
    "This function allows to load the datasets for training and testing.\n",
    "\n",
    "We are taking in this function the data already preprocessed notebook [Preprocessing](Preprocessing.ipynb), please run it first in order to work with the current notebook.\n",
    "\n",
    "this function returns two variables:\n",
    "* x_(train/test): this is a tensor with images\n",
    "* y_(train/test): array with zeros and ones  for labels. (ex: 0 = no tumor and 1 = glioma)\n",
    "\n",
    "to load the proper set of data we have to pass two parameters to the funtion:\n",
    "* dataset:  is a string with the value \"training\" or \"testing\"  to know which set are we loading.\n",
    "* _categories: two strings in an array to know which categories are we using (ex: [\"no_tumor\",\"glioma_tumor\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f875cfa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(dataset,_categories):\n",
    "    data = []\n",
    "    labels = []\n",
    "    path = \"\"\n",
    "    \n",
    "    if dataset != \"training\" or dataset == \"testing\":\n",
    "        print(f\"Error: invalid dataset type, options are 'training' or 'testing'\")\n",
    "        return\n",
    "    \n",
    "    if dataset == \"training\":\n",
    "        print(\"loading preprocessed training dataset\")\n",
    "        path = \"preprocessed/Training\"\n",
    "        \n",
    "    if dataset == \"testing\":\n",
    "        print(\"loading preprocessed testing dataset\")\n",
    "        path = \"preprocessed/Testing\"\n",
    "        \n",
    "    if len(_categories) != 2:\n",
    "        print(\"Error: please select two categories, this is for a binary classifier\")\n",
    "        return\n",
    "    \n",
    "    for category in _categories:\n",
    "        if category not in categories:\n",
    "            print(f\"Error: invalid category, options are {categories}\")\n",
    "            return \n",
    "        \n",
    "    for category in _categories:\n",
    "        label = _categories.index(category)\n",
    "        cat_path=f\"{path}/{category}\"\n",
    "        print(f\"loading category {category} from path {cat_path}\")\n",
    "        imgs_files = glob.glob(f\"{cat_path}/*\")\n",
    "        for img in imgs_files:\n",
    "            mat = np.load(img)\n",
    "            data.append(mat)\n",
    "            labels.append(label)\n",
    "    data = np.array(data).reshape((len(data),64,64,1))\n",
    "    labels = np.array(labels)\n",
    "    return data,labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fbf27de",
   "metadata": {},
   "source": [
    "# Loading the datasets\n",
    "\n",
    "In this section we are loading the datasets into the memory for traning and testing using our previuosly defined function.\n",
    "\n",
    "* x_(train/test): numpy tensor with images\n",
    "* y_(train/test): numpy array labels \n",
    "\n",
    "The dataset we are using is providing only sub set for training and testing, in order to have a validation sub set, we are splitting the test dataset into two subesets with scikit learn.\n",
    "\n",
    "Why validation subset? this is useful when you want to monitor the model in the training process. this allows to see for example if we have over training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b921d0c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train = load_dataset(\"training\",categories_selected)\n",
    "x_test, y_test = load_dataset(\"training\",categories_selected)\n",
    "\n",
    "#creating an aditional validation dataset \n",
    "x_valid, x_test, y_valid, y_test = train_test_split(x_test, y_test, test_size=0.5, shuffle= True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef52e454",
   "metadata": {},
   "source": [
    "# Summary of datasets\n",
    "\n",
    "Lets see some figures about our is distributed the subsets for training, validation and testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae53d280",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = (20,20)\n",
    "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(nrows=2, ncols=2)\n",
    "\n",
    "ax1.set_title('Subsets images')\n",
    "ax2.set_title('Training labels')\n",
    "ax3.set_title('Validation labels')\n",
    "ax4.set_title('Test labels')\n",
    "\n",
    "labels = ['training', 'validation', 'test']\n",
    "shapes = [x_train.shape[0],x_valid.shape[0],x_test.shape[0]]\n",
    "ax1.bar(labels,shapes)\n",
    "\n",
    "ax2.bar(categories_selected,[len(y_train)-np.count_nonzero(y_train),np.count_nonzero(y_train)])\n",
    "\n",
    "ax3.bar(categories_selected,[len(y_valid)-np.count_nonzero(y_valid),np.count_nonzero(y_valid)])\n",
    "\n",
    "ax4.bar(categories_selected,[len(y_test)-np.count_nonzero(y_test),np.count_nonzero(y_test)])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e62d65b0",
   "metadata": {},
   "source": [
    "# Defining the metrics for the model.\n",
    "\n",
    "There is a lot of metrics used in statistis to measure the performance of a model, for this exercise we took\n",
    "the most importants and widely used in medicine.\n",
    "\n",
    "1) the accuracy is the most popular for classification<a href=\"#11\">[11]</a> , it is defined as:\n",
    "$$\\text{Accuracy} = \\frac{TP+TN}{TP+TN+FP+FN}$$\n",
    "\n",
    "\n",
    "2) Sensitivity measures the proportion of actual positives that are correctly identified as such\n",
    "$$\\text{Sensitivity} = \\frac{TP}{TP+FN}$$\n",
    "\n",
    "3) Specificity measures the proportion of actual negatives that are correctly identified as such \n",
    "$$\\text{Sensitivity} = \\frac{TN}{TN+FP}$$\n",
    "\n",
    "\n",
    "4) Receiver operating characteristics (ROC), are commonly used in medical decision making <a href=\"#12\">[12]</a> \n",
    "let's take a look for a simple explanetion <a src=\"https://developers.google.com/machine-learning/crash-course/classification/roc-and-auc\">here</a>\n",
    "\n",
    "Where TP = True Positives, TN = True Negatives, FP = False Positives, and FN = False Negatives.\n",
    "\n",
    "\n",
    "\n",
    "The AUC (Area Under the Curve) for ROC and the accuracy are defined by default in tensorflow metrics, sensitivity and specificity aren't, then I defined then in funcitons that will be called in the training phase by tensorflow.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc00974e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sensitivity_caller():\n",
    "    _tp = tf.keras.metrics.TruePositives()\n",
    "    _fn = tf.keras.metrics.FalseNegatives()\n",
    "    @tf.function\n",
    "    def sensitivity(y_true, y_pred):\n",
    "        tp = _tp(y_true, y_pred)\n",
    "        fn = _fn(y_true, y_pred)\n",
    "        return tp/(tp+fn+K.epsilon())\n",
    "    return sensitivity\n",
    "\n",
    "def specificity_caller():\n",
    "    _tn = tf.keras.metrics.TrueNegatives()\n",
    "    _fp = tf.keras.metrics.FalsePositives()\n",
    "    @tf.function\n",
    "    def specificity(y_true, y_pred):\n",
    "        tn = _tn(y_true, y_pred)\n",
    "        fp = _fp(y_true, y_pred)\n",
    "        return tn/(tn+fp+K.epsilon())\n",
    "    return specificity\n",
    "\n",
    "metrics = [\n",
    "  tf.keras.metrics.BinaryAccuracy(name='accuracy'),\n",
    "  tf.keras.metrics.AUC(name='auc'), #ROC\n",
    "  sensitivity_caller(),\n",
    "  specificity_caller()\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "529567f8",
   "metadata": {},
   "source": [
    "# Machine Learning model\n",
    "\n",
    "The next step is to define the ML model using tensorflow, for this we are using a specialized architecture for image processing with convolutional neural networks.[https://confusedcoders.com/data-science/deep-learning/cnn-with-tensorflow-for-deep-learning-beginners](https://confusedcoders.com/data-science/deep-learning/cnn-with-tensorflow-for-deep-learning-beginners)\n",
    "\n",
    "<img src=\"http://rpmarchildon.com/wp-content/uploads/2018/06/RM-CNN-Schematic-1.jpg\" style=\"width:60%\"/>\n",
    "\n",
    "Convolutional neural networks are a set of layers that allow you to apply filters on images to ideantify the pattern.\n",
    "In order to define the model we need to define a Sequential object in tensorflow, with this object we are adding several layers defining the procedures along the sequence, the layers are the following:\n",
    "\n",
    "* Conv2D: Perform convolution for 2D images, in our case images of 64x64 with one channel\n",
    "<img src=\"https://miro.medium.com/max/700/1*ulfFYH5HbWpLTIfuebj5mQ.gif\"/>\n",
    "\n",
    "* Activation funciton, Rectified Linear Unit (ReLU): modify the output of the node.  [https://medium.com/@danqing/a-practical-guide-to-relu-b83ca804f1f7]:\n",
    "<img src=\"https://miro.medium.com/max/1400/1*DfMRHwxY1gyyDmrIAd-gjQ.png\" style=\"width:50%\"/>\n",
    "\n",
    "* AveragePooling2D:  is a pooling operation that calculates the average value for patches of a feature map. [https://paperswithcode.com/method/average-pooling]\n",
    "<img src=\"https://paperswithcode.com/media/methods/Screen_Shot_2020-05-24_at_1.51.40_PM.png\" style=\"width:20%\"/>\n",
    "\n",
    "\n",
    "images taken from https://towardsdatascience.com/convolutional-neural-networks-explained-9cc5188c4939\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4a17ab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "#hyperparameters\n",
    "dropout = 0.2\n",
    "n_filter = 2\n",
    "img_size=(64,64,1)\n",
    "optimizer_lr = 1e-3\n",
    "dense_neurons = 8\n",
    "\n",
    "\n",
    "model.add(Conv2D(2*n_filter, kernel_size=16, activation=tf.nn.relu, input_shape=img_size))\n",
    "model.add(Conv2D(4*n_filter, kernel_size=8))\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(AveragePooling2D(pool_size=(2,2), strides=None, padding='valid', data_format=None))\n",
    "\n",
    "model.add(Conv2D(4*n_filter, kernel_size=8))\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(AveragePooling2D(pool_size=(2, 2), strides=None, padding='valid', data_format=None))\n",
    "\n",
    "model.add(Conv2D(8*n_filter, kernel_size=4))\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(AveragePooling2D(pool_size=(2, 2), strides=None, padding='valid', data_format=None))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(dense_neurons, activation=tf.nn.relu))\n",
    "model.add(Dropout(dropout))\n",
    "model.add(Dense(1, activation=tf.nn.sigmoid))\n",
    "\n",
    "model.compile(optimizer=Adam(optimizer_lr),\n",
    "              loss='binary_crossentropy',metrics = metrics)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab5a2b12",
   "metadata": {},
   "source": [
    "# Train the model\n",
    "\n",
    "The next step is to train the model.\n",
    "There is some parameters we need to pass in order to train the model.\n",
    "\n",
    "* **x_(train/valid)**: arrays with images\n",
    "* **y_(train/valid)**: labels to let know to the model if the image is tumor or not (0: no tumor, 1: glioma)\n",
    "* **batch size**: the images are processed by batches this means how many images by batch. (so many images can increase a lot the RAM memory or the video memory if you are in a GPU accelerator)\n",
    "* **epochs**: number of times we will pass the information through the model to train it.\n",
    "\n",
    "the fit method returns the history of the traning, this allow to know more information about the training process and make some plots.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54b46e68",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 20\n",
    "batch_size = 10\n",
    "history = model.fit(x_train, y_train, validation_data=(x_valid, y_valid), \n",
    "                    batch_size=batch_size, epochs=epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a83b62c6",
   "metadata": {},
   "source": [
    "# Plotting the results of the training\n",
    "\n",
    "The next step is to make some useful plots to understand what happened in the training process.\n",
    "\n",
    "the plots will show information about the train and validation dataset, for the loss funciton and multiple matrics surch as AUC-ROC, accuracy, sensitivity and specificity.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12862361",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# list all data in history\n",
    "print(history.history.keys())\n",
    "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(nrows=2, ncols=2)\n",
    "\n",
    "ax1.set_title('Loss Training/Validation')\n",
    "ax2.set_title('Accuracy Training/Validation')\n",
    "ax3.set_title('Sensitivity Training/Validation')\n",
    "ax4.set_title('Specificity Training/Validation')\n",
    "\n",
    "# summarize history for accuracy\n",
    "ax1.plot(history.history['loss'])\n",
    "ax1.plot(history.history['val_loss'])\n",
    "\n",
    "ax2.plot(history.history['accuracy'])\n",
    "ax2.plot(history.history['val_accuracy'])\n",
    "\n",
    "ax3.plot(history.history['sensitivity'])\n",
    "ax3.plot(history.history['val_sensitivity'])\n",
    "\n",
    "ax4.plot(history.history['specificity'])\n",
    "ax4.plot(history.history['val_specificity'])\n",
    "\n",
    "ax1.set_xlabel('epoch')\n",
    "ax1.set_ylabel('loss')\n",
    "\n",
    "ax2.set_xlabel('epoch')\n",
    "ax2.set_ylabel('accuracy')\n",
    "\n",
    "ax3.set_xlabel('epoch')\n",
    "ax3.set_ylabel('sensitivity')\n",
    "\n",
    "ax4.set_xlabel('epoch')\n",
    "ax4.set_ylabel('specificity')\n",
    "\n",
    "\n",
    "ax1.legend(['train loss', 'valid loss'], loc='upper left')\n",
    "ax2.legend(['train acc','valid acc'], loc='upper left')\n",
    "ax3.legend(['train sen','valid sen'], loc='upper left')\n",
    "ax4.legend(['train spe','valid spe'], loc='upper left')\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b698960",
   "metadata": {},
   "source": [
    "# Analysis with the testing subset\n",
    "\n",
    "Let's validate the models with the testing subset, for this we are going to **make predictions** over the model with the test subset and we are going to plot the **confusion matrix**, **calculate ROC, sensitivity** and **specificity**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4472626",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict_classes(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b1a140b",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(nrows=2, ncols=2)\n",
    "\n",
    "ax1.set_title('Real Image with category {} predicted as {}'.format(y_test[0], y_pred[0][0]))\n",
    "ax1.imshow(x_test[0,:,:,0])\n",
    "\n",
    "ax2.set_title('Real Image with category {} predicted as {}'.format(y_test[1], y_pred[1][0]))\n",
    "ax2.imshow(x_test[1,:,:,0])\n",
    "\n",
    "ax3.set_title('Real Image with category {} predicted as {}'.format(y_test[2], y_pred[2][0]))\n",
    "ax3.imshow(x_test[2,:,:,0])\n",
    "\n",
    "ax4.set_title('Real Image with category {} predicted as {}'.format(y_test[3], y_pred[3][0]))\n",
    "ax4.imshow(x_test[3,:,:,0])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7850789d",
   "metadata": {},
   "source": [
    "# Analysing the confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69e81292",
   "metadata": {},
   "outputs": [],
   "source": [
    "tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
    "plot_confusion_matrix(cm           = confusion_matrix(y_test, y_pred), \n",
    "                      normalize    = False,\n",
    "                      target_names = categories_selected,\n",
    "                      title        = \"Confusion Matrix\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94089aa5",
   "metadata": {},
   "source": [
    "# Sensitivity and specificity\n",
    "\n",
    "Remember!\n",
    "* Sensitivity (True Positive rate) measures the proportion of positives that are correctly identified (i.e. the proportion of those who have some condition (affected) who are correctly identified as having the condition).\n",
    "* Specificity (True Negative rate) measures the proportion of negatives that are correctly identified (i.e. the proportion of those who do not have the condition (unaffected) who are correctly identified as not having the condition).\n",
    "\n",
    "\n",
    "**Sensitivity** = TP / (TP + FN) \n",
    "\n",
    "**Specificity** = TN / (TN + FP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e9c4c12",
   "metadata": {},
   "outputs": [],
   "source": [
    "sensitivity = tp / (tp + fn)\n",
    "specificity = tn / (tn + fp)\n",
    "\n",
    "print(f\"Sensitivity = {sensitivity} \")\n",
    "print(f\"Specificity = {specificity} \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "631c4ac8",
   "metadata": {},
   "source": [
    "\n",
    "## References\n",
    "<a id=\"1\">[1] </a> https://www.aans.org/en/Patients/Neurosurgical-Conditions-and-Treatments/Brain-Tumors\n",
    "\n",
    "<a id=\"2\">[2] </a> https://www.kaggle.com/sartajbhuvaji/brain-tumor-classification-mri\n",
    "\n",
    "<a id=\"3\">[3] </a>https://www.mdpi.com/2076-3417/10/6/1999\n",
    "\n",
    "<a id=\"4\">[4] </a>https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4879034/\n",
    "\n",
    "<a id=\"5\">[5] </a>https://github.com/ANTsX/ANTs\n",
    "\n",
    "<a id=\"6\">[6] </a>https://scikit-image.org/\n",
    "\n",
    "<a id=\"7\">[7] </a>https://opencv.org/\n",
    "\n",
    "<a id=\"8\">[8] </a>https://www.tensorflow.org/\n",
    "\n",
    "<a id=\"9\">[9] </a>https://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html\n",
    "\n",
    "<a id=\"10\">[10] </a>https://www.kaggle.com/grfiv4/plot-a-confusion-matrix\n",
    "\n",
    "<a id=\"11\">[11] </a>https://developers.google.com/machine-learning/crash-course/classification/accuracy\n",
    "\n",
    "<a id=\"12\">[12] </a> https://people.inf.elte.hu/kiss/11dwhdm/roc.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0152725",
   "metadata": {},
   "source": [
    "### Come back to the index\n",
    "\n",
    "Lets come back to the index to continue with the exercises!\n",
    "* [Index](index.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
