{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "590ddf59",
   "metadata": {},
   "source": [
    "<img style=\"width:20%;float: left; margin-right: 10px;\" src=\"https://upload.wikimedia.org/wikipedia/en/a/ae/CERN_logo.svg\"/>\n",
    "\n",
    "\n",
    "\n",
    "#    2D image classification using Machine Leaning for brain tumors\n",
    "\n",
    "## Introduction\n",
    "Brain tumors are considered by the medical community as an aggressive and difficult to treat disease that affects adults and children.\n",
    "\n",
    "The most common primary brain tumors are gliomas, meningiomas and pituitary.\n",
    "\n",
    "Gliomas are the most common type of brain tumor in adults. It is about 78 percent of malignant brain tumors according to the American Association of Neurological Surgeons. <a href=\"#1\"> [1] </a>\n",
    "\n",
    "In this tutorial we are going to classify 2D images with machine learning, for tumors such as **meningioma**, **glioma**, **pituitary** and people **without tumors**.\n",
    "\n",
    "As mentioned in the preprocessing notebook, we are using a public data set, taken from\n",
    "kaggle <a href=\"#2\"> [2] </a>, with 2D MRI images.\n",
    "\n",
    "## About ML for Brain Tumor Classification\n",
    "\n",
    "The classification of brain tumors is a complex task, it requires sophisticated machine learning and mathematical models, we are using the latest in the state of the art for image processing such as convolutional neural networks <a href=\"#3\"> [3] </a>.\n",
    "\n",
    "It also requires elaborated techniques to preprocess the images, such as skull stripping <a href=\"#4\"> [4] </a>, advanced tools for data normalization such as Ants <a href=\"#5\"> [5] </a>, tools for image transformation such as\n",
    "scikit-image <a href=\"#6\"> [6] </a> or open computer vision library (open-cv) <a href=\"#7\"> [7] </a>\n",
    "\n",
    "This tutorial is done using Tensorflow with Keras <a href=\"#8\"> [8] </a> and due to limited resources, we will run a binary classifier using 64x64 pixel 2D images.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59460cee",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "# Let's get started!\n",
    "\n",
    "The first step is to import the necessary modules. Those modules are for handling numerical matrices, plotting, creating the machine learning model and calculating some statistics about the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9000cfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "import glob\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Adam #http://arxiv.org/abs/1412.6980\n",
    "from tensorflow.keras.layers import Dense, Conv2D, Flatten, AveragePooling2D, Dropout,BatchNormalization,Activation,SpatialDropout2D\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "from sklearn.metrics import auc\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61772e0b",
   "metadata": {},
   "source": [
    "# Defining and Select the categories\n",
    "\n",
    "The second step is to define the categories for our problem, in our case it is an array of four labels for meningioma, glioma, pituitary tumors and no tumors.\n",
    "\n",
    "We have to select two of those categories to perform the binary classification. Initially we will take **no tumor** and **glioma**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b645cbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = ['meningioma_tumor', 'glioma_tumor', 'pituitary_tumor', 'no_tumor']\n",
    "categories_selected = [\"no_tumor\",\"glioma_tumor\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a22ff75e",
   "metadata": {},
   "source": [
    "# Function to plot a confusion matrix\n",
    "\n",
    "A confusion matrix is a table layout that allows to visualize the performance of the algorithm.<a href=\"#9\">[9]</a>\n",
    "\n",
    "In particular, this allows to see:\n",
    "* TP: True postive \n",
    "* TN: True negative\n",
    "* FP: False positive\n",
    "* FN: False negative\n",
    "\n",
    "We took this code snippet from kaggle <a href=\"#10\">[10]</a> to produce a nice plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca206ec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm,\n",
    "                          target_names,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=None,\n",
    "                          normalize=True):\n",
    "    accuracy = np.trace(cm) / float(np.sum(cm))\n",
    "    misclass = 1 - accuracy\n",
    "\n",
    "    if cmap is None:\n",
    "        cmap = plt.get_cmap('Blues')\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "\n",
    "    if target_names is not None:\n",
    "        tick_marks = np.arange(len(target_names))\n",
    "        plt.xticks(tick_marks, target_names, rotation=45)\n",
    "        plt.yticks(tick_marks, target_names)\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "\n",
    "    thresh = cm.max() / 1.5 if normalize else cm.max() / 2\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        if normalize:\n",
    "            plt.text(j, i, \"{:0.4f}\".format(cm[i, j]),\n",
    "                     horizontalalignment=\"center\",\n",
    "                     color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "        else:\n",
    "            plt.text(j, i, \"{:,}\".format(cm[i, j]),\n",
    "                     horizontalalignment=\"center\",\n",
    "                     color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label\\naccuracy={:0.4f}; misclass={:0.4f}'.format(accuracy, misclass))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "472ff84e",
   "metadata": {},
   "source": [
    "# Utility function to load the dataset\n",
    "\n",
    "This function allows you to load the data sets for training and testing.\n",
    "\n",
    "This function takes the already preprocessed data from the notebook [Preprocessing](Preprocessing.ipynb). Please run that notebook first to work with the current notebook.\n",
    "\n",
    "This function returns two variables:\n",
    "* x_ (train / test): this is a tensor with images\n",
    "* y_ (train / test): array with zeros and ones for labels. (for example: 0 = no tumor and 1 = glioma)\n",
    "\n",
    "To load the proper data set we have to pass two parameters to the function:\n",
    "* data set: it is a string with the value \"training\" or \"testing\" to know which set we are loading.\n",
    "* _categories: two strings in an array to know which categories we want to use (for example: [\"no_tumor\", \"glioma_tumor\"])."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f875cfa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(dataset,_categories):\n",
    "    data = []\n",
    "    labels = []\n",
    "    path = \"\"\n",
    "    \n",
    "    if dataset != \"training\" or dataset == \"testing\":\n",
    "        print(f\"Error: invalid dataset type, options are 'training' or 'testing'\")\n",
    "        return\n",
    "    \n",
    "    if dataset == \"training\":\n",
    "        print(\"loading preprocessed training dataset\")\n",
    "        path = \"preprocessed/Training\"\n",
    "        \n",
    "    if dataset == \"testing\":\n",
    "        print(\"loading preprocessed testing dataset\")\n",
    "        path = \"preprocessed/Testing\"\n",
    "        \n",
    "    if len(_categories) != 2:\n",
    "        print(\"Error: please select two categories, this is for a binary classifier\")\n",
    "        return\n",
    "    \n",
    "    for category in _categories:\n",
    "        if category not in categories:\n",
    "            print(f\"Error: invalid category, options are {categories}\")\n",
    "            return \n",
    "        \n",
    "    for category in _categories:\n",
    "        label = _categories.index(category)\n",
    "        cat_path=f\"{path}/{category}\"\n",
    "        print(f\"loading category {category} from path {cat_path}\")\n",
    "        imgs_files = glob.glob(f\"{cat_path}/*\")\n",
    "        for img in imgs_files:\n",
    "            mat = np.load(img)\n",
    "            data.append(mat)\n",
    "            labels.append(label)\n",
    "    data = np.array(data).reshape((len(data),64,64,1))\n",
    "    labels = np.array(labels)\n",
    "    return data,labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fbf27de",
   "metadata": {},
   "source": [
    "# Loading the datasets\n",
    "\n",
    "In this section, we are loading the data sets into memory for training and testing using our previously defined function.\n",
    "\n",
    "* x_ (train / test): numpy tensor with images\n",
    "* y_ (train / test): numerous array labels\n",
    "\n",
    "The dataset we are using provides only a subset for training and testing. To have a validation subset, we are dividing the test dataset into two subsets with scikit learn.\n",
    "\n",
    "Why a validation subset? This is useful when you want to monitor the model in the training process. This allows us to see for example if we have overtraining."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b921d0c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train = load_dataset(\"training\",categories_selected)\n",
    "x_test, y_test = load_dataset(\"training\",categories_selected)\n",
    "\n",
    "#creating an additional validation dataset \n",
    "x_valid, x_test, y_valid, y_test = train_test_split(x_test, y_test, test_size=0.5, shuffle= True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef52e454",
   "metadata": {},
   "source": [
    "# Summary of datasets\n",
    "\n",
    "Let's look at some figures on how subsets are distributed for training, validation and testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae53d280",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = (20,20)\n",
    "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(nrows=2, ncols=2)\n",
    "\n",
    "ax1.set_title('Subsets images')\n",
    "ax2.set_title('Training labels')\n",
    "ax3.set_title('Validation labels')\n",
    "ax4.set_title('Test labels')\n",
    "\n",
    "labels = ['training', 'validation', 'test']\n",
    "shapes = [x_train.shape[0],x_valid.shape[0],x_test.shape[0]]\n",
    "ax1.bar(labels,shapes)\n",
    "\n",
    "ax2.bar(categories_selected,[len(y_train)-np.count_nonzero(y_train),np.count_nonzero(y_train)])\n",
    "\n",
    "ax3.bar(categories_selected,[len(y_valid)-np.count_nonzero(y_valid),np.count_nonzero(y_valid)])\n",
    "\n",
    "ax4.bar(categories_selected,[len(y_test)-np.count_nonzero(y_test),np.count_nonzero(y_test)])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e62d65b0",
   "metadata": {},
   "source": [
    "# Defining the metrics for the model.\n",
    "\n",
    "There are many metrics that are used in statistics to measure the performance of a model, for this exercise we take the most important and widely used in medicine.\n",
    "\n",
    "1) The accuracy is the most popular for classification<a href=\"#11\">[11]</a> , it is defined as:\n",
    "$$\\text{Accuracy} = \\frac{TP+TN}{TP+TN+FP+FN}$$\n",
    "\n",
    "2) Sensitivity (True Positive rate) measures the proportion of positives that are correctly identified (i.e. the proportion of those who have some condition (affected) and are correctly identified as having the condition).\n",
    "$$\\text{Sensitivity} = \\frac{TP}{TP+FN}$$\n",
    "\n",
    "3) Specificity (True Negative rate) measures the proportion of negatives that are correctly identified (i.e. the proportion of those who do not have the condition (unaffected) and are correctly identified as not having the condition).\n",
    "$$\\text{Specificity} = \\frac{TN}{TN+FP}$$\n",
    "\n",
    "4) Receiver operating characteristics (ROC), they are commonly used in medical decision making. <a href=\"#12\">[12]</a> \n",
    "Let's take a look to a simple explanation <a href=\"https://developers.google.com/machine-learning/crash-course/classification/roc-and-auc\">here</a>.\n",
    "\n",
    "Where TP = True Positives, TN = True Negatives, FP = False Positives and FN = False Negatives.\n",
    "\n",
    "The AUC (Area Under the Curve) for ROC and the precision are defined by default in the tensorflow metrics, the sensitivity and specificity are not, for that we define them in functions that will be called in the training phase by tensorflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc00974e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sensitivity_caller():\n",
    "    _tp = tf.keras.metrics.TruePositives()\n",
    "    _fn = tf.keras.metrics.FalseNegatives()\n",
    "    @tf.function\n",
    "    def sensitivity(y_true, y_pred):\n",
    "        tp = _tp(y_true, y_pred)\n",
    "        fn = _fn(y_true, y_pred)\n",
    "        return tp/(tp+fn+K.epsilon())\n",
    "    return sensitivity\n",
    "\n",
    "def specificity_caller():\n",
    "    _tn = tf.keras.metrics.TrueNegatives()\n",
    "    _fp = tf.keras.metrics.FalsePositives()\n",
    "    @tf.function\n",
    "    def specificity(y_true, y_pred):\n",
    "        tn = _tn(y_true, y_pred)\n",
    "        fp = _fp(y_true, y_pred)\n",
    "        return tn/(tn+fp+K.epsilon())\n",
    "    return specificity\n",
    "\n",
    "metrics = [\n",
    "  tf.keras.metrics.BinaryAccuracy(name='accuracy'),\n",
    "  tf.keras.metrics.AUC(name='auc'), #ROC\n",
    "  sensitivity_caller(),\n",
    "  specificity_caller()\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "529567f8",
   "metadata": {},
   "source": [
    "# Machine Learning model\n",
    "\n",
    "The next step is to define the ML model using tensorflow, for this we are using a specialized architecture for image processing with convolutional neural networks. [Good explanation here](https://confusedcoders.com/data-science/deep-learning/cnn-with-tensorflow-for-deep-learning-beginners)\n",
    "\n",
    "<img src=\"http://rpmarchildon.com/wp-content/uploads/2018/06/RM-CNN-Schematic-1.jpg\" style=\"width:60%\"/>\n",
    "\n",
    "Convolutional neural networks are a set of layers that allow you to apply filters to images to identify patterns.\n",
    "In order to define the model we need to define a Sequential object in tensorflow, with this object we are adding several layers defining the procedures throughout the sequence. The layers are the following:\n",
    "\n",
    "* Conv2D: Perform convolution for 2D images. In our case, images of 64x64 with one channel.\n",
    "<img src=\"https://miro.medium.com/max/700/1*ulfFYH5HbWpLTIfuebj5mQ.gif\"/>\n",
    "\n",
    "* Activation function, Rectified Linear Unit (ReLU): modifies the output of the node.  [https://medium.com/@danqing/a-practical-guide-to-relu-b83ca804f1f7]:\n",
    "<img src=\"https://miro.medium.com/max/1400/1*DfMRHwxY1gyyDmrIAd-gjQ.png\" style=\"width:50%\"/>\n",
    "\n",
    "* AveragePooling2D: pooling operation that calculates the average value of the patches. [https://paperswithcode.com/method/average-pooling]\n",
    "<img src=\"https://paperswithcode.com/media/methods/Screen_Shot_2020-05-24_at_1.51.40_PM.png\" style=\"width:20%\"/>\n",
    "\n",
    "\n",
    "images taken from <a href=\"#13\">[13]</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4a17ab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "#hyperparameters\n",
    "dropout = 0.2\n",
    "n_filter = 2\n",
    "img_size=(64,64,1)\n",
    "optimizer_lr = 1e-3\n",
    "dense_neurons = 8\n",
    "\n",
    "\n",
    "model.add(Conv2D(2*n_filter, kernel_size=16, activation=tf.nn.relu, input_shape=img_size))\n",
    "model.add(Conv2D(4*n_filter, kernel_size=8))\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(AveragePooling2D(pool_size=(2,2), strides=None, padding='valid', data_format=None))\n",
    "\n",
    "model.add(Conv2D(4*n_filter, kernel_size=8))\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(AveragePooling2D(pool_size=(2, 2), strides=None, padding='valid', data_format=None))\n",
    "\n",
    "model.add(Conv2D(8*n_filter, kernel_size=4))\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(AveragePooling2D(pool_size=(2, 2), strides=None, padding='valid', data_format=None))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(dense_neurons, activation=tf.nn.relu))\n",
    "model.add(Dropout(dropout))\n",
    "model.add(Dense(1, activation=tf.nn.sigmoid))\n",
    "\n",
    "model.compile(optimizer=Adam(optimizer_lr),\n",
    "              loss='binary_crossentropy',metrics = metrics)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab5a2b12",
   "metadata": {},
   "source": [
    "# Train the model\n",
    "\n",
    "The next step is to train the model.\n",
    "There are some parameters we need to provide in order to train the model.\n",
    "\n",
    "* **x_(train/valid)**: arrays with images\n",
    "* **y_(train/valid)**: labels to let know to the model if the image is tumor or not (0: no tumor, 1: glioma)\n",
    "* **batch size**: the images are processed by batches. This parameter specifies how many images we want per batch. The number of images affects the RAM memory or the video memory if you are in a GPU accelerator\n",
    "* **epochs**: number of times we will pass the information through the model to train it\n",
    "\n",
    "The fit method returns the history of the traning. This allows to know more information about the training process and make some plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54b46e68",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 20\n",
    "batch_size = 10\n",
    "history = model.fit(x_train, y_train, validation_data=(x_valid, y_valid), \n",
    "                    batch_size=batch_size, epochs=epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a83b62c6",
   "metadata": {},
   "source": [
    "# Plotting the results of the training\n",
    "\n",
    "The next step is to make some useful plots to understand what happened in the training process.\n",
    "\n",
    "The plots will show information about the train and validation datasets. Moreover, they also show the loss function and multiple metrics such as accuracy, sensitivity and specificity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12862361",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# list all data in history\n",
    "#print(history.history.keys())\n",
    "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(nrows=2, ncols=2)\n",
    "\n",
    "ax1.set_title('Loss Training/Validation')\n",
    "ax2.set_title('Accuracy Training/Validation')\n",
    "ax3.set_title('Sensitivity Training/Validation')\n",
    "ax4.set_title('Specificity Training/Validation')\n",
    "\n",
    "# summarize history for accuracy\n",
    "ax1.plot(history.history['loss'])\n",
    "ax1.plot(history.history['val_loss'])\n",
    "\n",
    "ax2.plot(history.history['accuracy'])\n",
    "ax2.plot(history.history['val_accuracy'])\n",
    "\n",
    "ax3.plot(history.history['sensitivity'])\n",
    "ax3.plot(history.history['val_sensitivity'])\n",
    "\n",
    "ax4.plot(history.history['specificity'])\n",
    "ax4.plot(history.history['val_specificity'])\n",
    "\n",
    "ax1.set_xlabel('epoch')\n",
    "ax1.set_ylabel('loss')\n",
    "\n",
    "ax2.set_xlabel('epoch')\n",
    "ax2.set_ylabel('accuracy')\n",
    "\n",
    "ax3.set_xlabel('epoch')\n",
    "ax3.set_ylabel('sensitivity')\n",
    "\n",
    "ax4.set_xlabel('epoch')\n",
    "ax4.set_ylabel('specificity')\n",
    "\n",
    "\n",
    "ax1.legend(['train loss', 'valid loss'], loc='upper left')\n",
    "ax2.legend(['train acc','valid acc'], loc='upper left')\n",
    "ax3.legend(['train sen','valid sen'], loc='upper left')\n",
    "ax4.legend(['train spe','valid spe'], loc='upper left')\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b698960",
   "metadata": {},
   "source": [
    "# Analysis with the testing subset\n",
    "\n",
    "Let's validate the models with the testing subset, for this we are going to **make predictions** over the model with the testing subset and we are going to plot the **confusion matrix**, **sensitivity** and **specificity**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4472626",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict_classes(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b1a140b",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(nrows=2, ncols=2)\n",
    "\n",
    "ax1.set_title('Real Image with category {} predicted as {}'.format(y_test[0], y_pred[0][0]))\n",
    "ax1.imshow(x_test[0,:,:,0])\n",
    "\n",
    "ax2.set_title('Real Image with category {} predicted as {}'.format(y_test[1], y_pred[1][0]))\n",
    "ax2.imshow(x_test[1,:,:,0])\n",
    "\n",
    "ax3.set_title('Real Image with category {} predicted as {}'.format(y_test[2], y_pred[2][0]))\n",
    "ax3.imshow(x_test[2,:,:,0])\n",
    "\n",
    "ax4.set_title('Real Image with category {} predicted as {}'.format(y_test[3], y_pred[3][0]))\n",
    "ax4.imshow(x_test[3,:,:,0])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7850789d",
   "metadata": {},
   "source": [
    "# Analysing the confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69e81292",
   "metadata": {},
   "outputs": [],
   "source": [
    "tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
    "plot_confusion_matrix(cm           = confusion_matrix(y_test, y_pred), \n",
    "                      normalize    = False,\n",
    "                      target_names = categories_selected,\n",
    "                      title        = \"Confusion Matrix\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94089aa5",
   "metadata": {},
   "source": [
    "# Sensitivity and specificity\n",
    "\n",
    "Remember!\n",
    "* Sensitivity (True Positive rate) measures the proportion of positives that are correctly identified (i.e. the proportion of those and have some condition (affected) who are correctly identified as having the condition).\n",
    "* Specificity (True Negative rate) measures the proportion of negatives that are correctly identified (i.e. the proportion of those and do not have the condition (unaffected) who are correctly identified as not having the condition).\n",
    "\n",
    "**Sensitivity** = TP / (TP + FN) \n",
    "\n",
    "**Specificity** = TN / (TN + FP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e9c4c12",
   "metadata": {},
   "outputs": [],
   "source": [
    "sensitivity = tp / (tp + fn)\n",
    "specificity = tn / (tn + fp)\n",
    "\n",
    "print(f\"Sensitivity = {sensitivity} \")\n",
    "print(f\"Specificity = {specificity} \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "631c4ac8",
   "metadata": {},
   "source": [
    "\n",
    "## References\n",
    "<a id=\"1\">[1] </a> https://www.aans.org/en/Patients/Neurosurgical-Conditions-and-Treatments/Brain-Tumors\n",
    "\n",
    "<a id=\"2\">[2] </a> https://www.kaggle.com/sartajbhuvaji/brain-tumor-classification-mri\n",
    "\n",
    "<a id=\"3\">[3] </a>https://www.mdpi.com/2076-3417/10/6/1999\n",
    "\n",
    "<a id=\"4\">[4] </a>https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4879034/\n",
    "\n",
    "<a id=\"5\">[5] </a>https://github.com/ANTsX/ANTs\n",
    "\n",
    "<a id=\"6\">[6] </a>https://scikit-image.org/\n",
    "\n",
    "<a id=\"7\">[7] </a>https://opencv.org/\n",
    "\n",
    "<a id=\"8\">[8] </a>https://www.tensorflow.org/\n",
    "\n",
    "<a id=\"9\">[9] </a>https://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html\n",
    "\n",
    "<a id=\"10\">[10] </a>https://www.kaggle.com/grfiv4/plot-a-confusion-matrix\n",
    "\n",
    "<a id=\"11\">[11] </a>https://developers.google.com/machine-learning/crash-course/classification/accuracy\n",
    "\n",
    "<a id=\"12\">[12] </a> https://people.inf.elte.hu/kiss/11dwhdm/roc.pdf\n",
    "\n",
    "<a id=\"13\">[13] </a>https://towardsdatascience.com/convolutional-neural-networks-explained-9cc5188c4939"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0152725",
   "metadata": {},
   "source": [
    "### Come back to the index\n",
    "\n",
    "Let's come back to the index to continue with the exercises!\n",
    "* [Index](index.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
